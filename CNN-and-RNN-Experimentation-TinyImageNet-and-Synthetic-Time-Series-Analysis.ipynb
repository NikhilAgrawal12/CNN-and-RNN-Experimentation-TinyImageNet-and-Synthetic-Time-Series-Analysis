{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e8cdac-8e3a-43a8-9ead-d1b5ea9dcac1",
   "metadata": {},
   "source": [
    "## CNN on tinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19206633-f06a-4673-9eb9-617aa563e320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating VGG-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG-19 Accuracy: 17.52%\n",
      "Evaluating ResNet-50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-50 Accuracy: 15.11%\n",
      "Evaluating InceptionV4...\n",
      "InceptionV4 Accuracy: 2.55%\n",
      "\n",
      "Comparison Report:\n",
      "VGG-19 Accuracy: 17.52%\n",
      "ResNet-50 Accuracy: 15.11%\n",
      "InceptionV4 Accuracy: 2.55%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.models import vgg19, resnet50\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import timm  \n",
    "import os\n",
    "from shutil import copy2\n",
    "\n",
    "# Load Tiny ImageNet dataset directory path\n",
    "data_dir = 'tiny-imagenet-200'\n",
    "\n",
    "# Organize validation data based on annotations\n",
    "def organize_val_data(data_dir):\n",
    "    val_annotations_path = os.path.join(data_dir, 'val', 'val_annotations.txt')\n",
    "    val_images_path = os.path.join(data_dir, 'val', 'images')\n",
    "    val_target_dir = os.path.join(data_dir, 'val', 'organized')\n",
    "\n",
    "    os.makedirs(val_target_dir, exist_ok=True)\n",
    "\n",
    "    with open(val_annotations_path, 'r') as file:\n",
    "        for line in file:\n",
    "            split_line = line.split('\\t')\n",
    "            img_file = split_line[0]\n",
    "            label = split_line[1]\n",
    "\n",
    "            label_dir = os.path.join(val_target_dir, label)\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "            img_path = os.path.join(val_images_path, img_file)\n",
    "            copy2(img_path, os.path.join(label_dir, img_file))\n",
    "\n",
    "    return val_target_dir\n",
    "\n",
    "val_target_dir = organize_val_data(data_dir)\n",
    "\n",
    "# Data preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Reduce image size for faster processing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
    "])\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_dataset = datasets.ImageFolder(root=f'{data_dir}/train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_target_dir, transform=transform)\n",
    "\n",
    "# Optional: Use a subset of data for faster experimentation\n",
    "subset_indices = random.sample(range(len(train_dataset)), 2000)  # Adjust as needed for faster processing\n",
    "train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# Function to evaluate model accuracy\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to fine-tune and evaluate a model\n",
    "def fine_tune_model(model, train_loader, val_loader, device, num_epochs=1):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = evaluate_model(model, val_loader, device)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate VGG-19\n",
    "print(\"Evaluating VGG-19...\")\n",
    "vgg19_model = vgg19(pretrained=True)\n",
    "vgg19_model.classifier[6] = nn.Linear(4096, 200)  # Adjust output layer for 200 classes\n",
    "vgg19_accuracy = fine_tune_model(vgg19_model, train_loader, val_loader, device)\n",
    "print(f\"VGG-19 Accuracy: {vgg19_accuracy:.2f}%\")\n",
    "\n",
    "# Evaluate ResNet-50\n",
    "print(\"Evaluating ResNet-50...\")\n",
    "resnet50_model = resnet50(pretrained=True)\n",
    "resnet50_model.fc = nn.Linear(resnet50_model.fc.in_features, 200)  # Adjust output layer\n",
    "resnet50_accuracy = fine_tune_model(resnet50_model, train_loader, val_loader, device)\n",
    "print(f\"ResNet-50 Accuracy: {resnet50_accuracy:.2f}%\")\n",
    "\n",
    "# Load and fine-tune InceptionV4 using `timm`\n",
    "print(\"Evaluating InceptionV4...\")\n",
    "inceptionv4_model = timm.create_model('inception_v4', pretrained=True)  # Load pre-trained InceptionV4\n",
    "inceptionv4_model.reset_classifier(num_classes=200)  # Adjust output layer for 200 classes\n",
    "\n",
    "# Only enable gradients for the classifier layer\n",
    "for param in inceptionv4_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in inceptionv4_model.get_classifier().parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Fine-tune the model\n",
    "inceptionv4_accuracy = fine_tune_model(inceptionv4_model, train_loader, val_loader, device)\n",
    "print(f\"InceptionV4 Accuracy: {inceptionv4_accuracy:.2f}%\")\n",
    "\n",
    "# Report results\n",
    "print(\"\\nComparison Report:\")\n",
    "print(f\"VGG-19 Accuracy: {vgg19_accuracy:.2f}%\")\n",
    "print(f\"ResNet-50 Accuracy: {resnet50_accuracy:.2f}%\")\n",
    "print(f\"InceptionV4 Accuracy: {inceptionv4_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe040f-6b72-472a-87fb-0a99ccfb25dc",
   "metadata": {},
   "source": [
    "## RNN on synthetic time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "517ef327-4232-4bf5-9538-418367c8d7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiments for dataset size: 1000\n",
      "\n",
      "Training LSTM on dataset of size 1000...\n",
      "Epoch 1/10, Loss: 0.3432\n",
      "Epoch 2/10, Loss: 0.3367\n",
      "Epoch 3/10, Loss: 0.3304\n",
      "Epoch 4/10, Loss: 0.3244\n",
      "Epoch 5/10, Loss: 0.3185\n",
      "Epoch 6/10, Loss: 0.3129\n",
      "Epoch 7/10, Loss: 0.3073\n",
      "Epoch 8/10, Loss: 0.3019\n",
      "Epoch 9/10, Loss: 0.2964\n",
      "Epoch 10/10, Loss: 0.2910\n",
      "Validation Loss: 0.3131\n",
      "\n",
      "Training GRU on dataset of size 1000...\n",
      "Epoch 1/10, Loss: 0.3170\n",
      "Epoch 2/10, Loss: 0.3040\n",
      "Epoch 3/10, Loss: 0.2913\n",
      "Epoch 4/10, Loss: 0.2789\n",
      "Epoch 5/10, Loss: 0.2667\n",
      "Epoch 6/10, Loss: 0.2548\n",
      "Epoch 7/10, Loss: 0.2432\n",
      "Epoch 8/10, Loss: 0.2318\n",
      "Epoch 9/10, Loss: 0.2206\n",
      "Epoch 10/10, Loss: 0.2096\n",
      "Validation Loss: 0.2196\n",
      "\n",
      "Training Bidirectional RNN on dataset of size 1000...\n",
      "Epoch 1/10, Loss: 0.3312\n",
      "Epoch 2/10, Loss: 0.3256\n",
      "Epoch 3/10, Loss: 0.3200\n",
      "Epoch 4/10, Loss: 0.3146\n",
      "Epoch 5/10, Loss: 0.3092\n",
      "Epoch 6/10, Loss: 0.3039\n",
      "Epoch 7/10, Loss: 0.2986\n",
      "Epoch 8/10, Loss: 0.2933\n",
      "Epoch 9/10, Loss: 0.2880\n",
      "Epoch 10/10, Loss: 0.2827\n",
      "Validation Loss: 0.3056\n",
      "\n",
      "Training Deep RNN on dataset of size 1000...\n",
      "Epoch 1/10, Loss: 0.3278\n",
      "Epoch 2/10, Loss: 0.2939\n",
      "Epoch 3/10, Loss: 0.2625\n",
      "Epoch 4/10, Loss: 0.2317\n",
      "Epoch 5/10, Loss: 0.1994\n",
      "Epoch 6/10, Loss: 0.1640\n",
      "Epoch 7/10, Loss: 0.1248\n",
      "Epoch 8/10, Loss: 0.0829\n",
      "Epoch 9/10, Loss: 0.0427\n",
      "Epoch 10/10, Loss: 0.0142\n",
      "Validation Loss: 0.0123\n",
      "\n",
      "Running experiments for dataset size: 3000\n",
      "\n",
      "Training LSTM on dataset of size 3000...\n",
      "Epoch 1/10, Loss: 0.3008\n",
      "Epoch 2/10, Loss: 0.2947\n",
      "Epoch 3/10, Loss: 0.2885\n",
      "Epoch 4/10, Loss: 0.2820\n",
      "Epoch 5/10, Loss: 0.2754\n",
      "Epoch 6/10, Loss: 0.2685\n",
      "Epoch 7/10, Loss: 0.2613\n",
      "Epoch 8/10, Loss: 0.2538\n",
      "Epoch 9/10, Loss: 0.2458\n",
      "Epoch 10/10, Loss: 0.2374\n",
      "Validation Loss: 0.2232\n",
      "\n",
      "Training GRU on dataset of size 3000...\n",
      "Epoch 1/10, Loss: 0.2097\n",
      "Epoch 2/10, Loss: 0.1982\n",
      "Epoch 3/10, Loss: 0.1868\n",
      "Epoch 4/10, Loss: 0.1755\n",
      "Epoch 5/10, Loss: 0.1644\n",
      "Epoch 6/10, Loss: 0.1533\n",
      "Epoch 7/10, Loss: 0.1424\n",
      "Epoch 8/10, Loss: 0.1317\n",
      "Epoch 9/10, Loss: 0.1211\n",
      "Epoch 10/10, Loss: 0.1106\n",
      "Validation Loss: 0.0985\n",
      "\n",
      "Training Bidirectional RNN on dataset of size 3000...\n",
      "Epoch 1/10, Loss: 0.2921\n",
      "Epoch 2/10, Loss: 0.2861\n",
      "Epoch 3/10, Loss: 0.2801\n",
      "Epoch 4/10, Loss: 0.2739\n",
      "Epoch 5/10, Loss: 0.2675\n",
      "Epoch 6/10, Loss: 0.2608\n",
      "Epoch 7/10, Loss: 0.2540\n",
      "Epoch 8/10, Loss: 0.2468\n",
      "Epoch 9/10, Loss: 0.2393\n",
      "Epoch 10/10, Loss: 0.2314\n",
      "Validation Loss: 0.2180\n",
      "\n",
      "Training Deep RNN on dataset of size 3000...\n",
      "Epoch 1/10, Loss: 0.0117\n",
      "Epoch 2/10, Loss: 0.0179\n",
      "Epoch 3/10, Loss: 0.0147\n",
      "Epoch 4/10, Loss: 0.0085\n",
      "Epoch 5/10, Loss: 0.0104\n",
      "Epoch 6/10, Loss: 0.0131\n",
      "Epoch 7/10, Loss: 0.0099\n",
      "Epoch 8/10, Loss: 0.0079\n",
      "Epoch 9/10, Loss: 0.0090\n",
      "Epoch 10/10, Loss: 0.0104\n",
      "Validation Loss: 0.0105\n",
      "\n",
      "Running experiments for dataset size: 9000\n",
      "\n",
      "Training LSTM on dataset of size 9000...\n",
      "Epoch 1/10, Loss: 0.2142\n",
      "Epoch 2/10, Loss: 0.2050\n",
      "Epoch 3/10, Loss: 0.1951\n",
      "Epoch 4/10, Loss: 0.1843\n",
      "Epoch 5/10, Loss: 0.1726\n",
      "Epoch 6/10, Loss: 0.1598\n",
      "Epoch 7/10, Loss: 0.1457\n",
      "Epoch 8/10, Loss: 0.1304\n",
      "Epoch 9/10, Loss: 0.1139\n",
      "Epoch 10/10, Loss: 0.0962\n",
      "Validation Loss: 0.0778\n",
      "\n",
      "Training GRU on dataset of size 9000...\n",
      "Epoch 1/10, Loss: 0.0938\n",
      "Epoch 2/10, Loss: 0.0830\n",
      "Epoch 3/10, Loss: 0.0723\n",
      "Epoch 4/10, Loss: 0.0616\n",
      "Epoch 5/10, Loss: 0.0510\n",
      "Epoch 6/10, Loss: 0.0408\n",
      "Epoch 7/10, Loss: 0.0311\n",
      "Epoch 8/10, Loss: 0.0223\n",
      "Epoch 9/10, Loss: 0.0149\n",
      "Epoch 10/10, Loss: 0.0096\n",
      "Validation Loss: 0.0070\n",
      "\n",
      "Training Bidirectional RNN on dataset of size 9000...\n",
      "Epoch 1/10, Loss: 0.2090\n",
      "Epoch 2/10, Loss: 0.2005\n",
      "Epoch 3/10, Loss: 0.1913\n",
      "Epoch 4/10, Loss: 0.1813\n",
      "Epoch 5/10, Loss: 0.1705\n",
      "Epoch 6/10, Loss: 0.1586\n",
      "Epoch 7/10, Loss: 0.1456\n",
      "Epoch 8/10, Loss: 0.1314\n",
      "Epoch 9/10, Loss: 0.1160\n",
      "Epoch 10/10, Loss: 0.0995\n",
      "Validation Loss: 0.0811\n",
      "\n",
      "Training Deep RNN on dataset of size 9000...\n",
      "Epoch 1/10, Loss: 0.0092\n",
      "Epoch 2/10, Loss: 0.0204\n",
      "Epoch 3/10, Loss: 0.0083\n",
      "Epoch 4/10, Loss: 0.0093\n",
      "Epoch 5/10, Loss: 0.0134\n",
      "Epoch 6/10, Loss: 0.0125\n",
      "Epoch 7/10, Loss: 0.0091\n",
      "Epoch 8/10, Loss: 0.0072\n",
      "Epoch 9/10, Loss: 0.0086\n",
      "Epoch 10/10, Loss: 0.0104\n",
      "Validation Loss: 0.0095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generating synthetic time series data\n",
    "def generate_synthetic_time_series(size=1000):\n",
    "    x = np.linspace(0, 4 * np.pi, size)\n",
    "    y = np.sin(x) + 0.1 * np.random.randn(size)\n",
    "    return y\n",
    "\n",
    "# Create dataset\n",
    "def create_dataset(series, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - seq_length):\n",
    "        X.append(series[i:i+seq_length])\n",
    "        y.append(series[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare data for training\n",
    "def prepare_data(series, seq_length):\n",
    "    X, y = create_dataset(series, seq_length)\n",
    "    X = X.reshape(-1, seq_length, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Normalize and prepare data\n",
    "def prepare_time_series_data(size, seq_length):\n",
    "    series = generate_synthetic_time_series(size=size)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    series = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "    X, y = prepare_data(series, seq_length)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = map(torch.tensor, (X_train, X_val, y_train, y_val))\n",
    "    return X_train.float(), X_val.float(), y_train.float(), y_val.float()\n",
    "\n",
    "# Define model architectures\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, model_type='lstm'):\n",
    "        super(RNNModel, self).__init__()\n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif model_type == 'gru':\n",
    "            self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        elif model_type == 'bidir':\n",
    "            self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if model_type == 'bidir' else 1), output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "        return self.fc(h[:, -1, :])\n",
    "\n",
    "# Training function\n",
    "def train_model(model, X_train, y_train, num_epochs=10, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation function on testing data\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    print(f\"Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Models for training\n",
    "models = {\n",
    "    \"LSTM\": RNNModel(input_dim=1, hidden_dim=50, output_dim=1, model_type='lstm'),\n",
    "    \"GRU\": RNNModel(input_dim=1, hidden_dim=50, output_dim=1, model_type='gru'),\n",
    "    \"Bidirectional RNN\": RNNModel(input_dim=1, hidden_dim=50, output_dim=1, model_type='bidir'),\n",
    "    \"Deep RNN\": RNNModel(input_dim=1, hidden_dim=50, output_dim=1, num_layers=2, model_type='rnn')\n",
    "}\n",
    "\n",
    "# Experiment parameters\n",
    "seq_length = 20\n",
    "sizes = [1000, 3000, 3000 * 3]  # Original, three times, and nine times sizes\n",
    "\n",
    "# Training loop for each size\n",
    "for size in sizes:\n",
    "    print(f\"\\nRunning experiments for dataset size: {size}\")\n",
    "    X_train, X_val, y_train, y_val = prepare_time_series_data(size, seq_length)\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} on dataset of size {size}...\")\n",
    "        train_model(model, X_train, y_train)\n",
    "        evaluate_model(model, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a57e32-2e40-4ace-8bff-fb6d9fbb8d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
